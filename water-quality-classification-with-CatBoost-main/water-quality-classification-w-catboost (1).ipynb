{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<b>The dataset is very unbalanced. </b> For example, C3S4 and C3S4 classes have only one sample. This situation is not good in respect of  using the oversampling methods. <br>\n\nI tried to create a new class (called the other) by combining other classes which have a small sample size.\nFinally, there are six classes to classify, named 'C2S1', 'C3S1', 'C3S2', 'C4S1', 'C4S2', 'Other'.\n\n\n## Results\n\nI have used optima for optimizing the model. The results were not very good. Although our model gained reasonable precision/recall scores in a few classes, some classes have bad scores.<br>\n\n- (Test Set) R2 score : 82.851\n- (Test Set) MAE : 0.207831\n\n| class        \t| precision \t| recall \t| f1-score \t| support \t|\n|--------------\t|-----------\t|--------\t|----------\t|---------\t|\n| 0            \t| 1.00      \t| 0.97   \t| 0.99     \t| 76      \t|\n| 1            \t| 0.97      \t| 0.99   \t| 0.98     \t| 204     \t|\n| 2            \t| 0.25      \t| 0.17   \t| 0.20     \t| 6       \t|\n| 3            \t| 0.88      \t| 0.88   \t| 0.88     \t| 26      \t|\n| 4            \t| 0.73      \t| 0.67   \t| 0.70     \t| 12      \t|\n| 5            \t| 0.62      \t| 0.62   \t| 0.62     \t| 8       \t|\n| accuracy     \t|           \t|        \t| 0.94     \t| 332     \t|\n| macro avg    \t| 0.74      \t| 0.72   \t| 0.73     \t| 332     \t|\n| weighted avg \t| 0.94      \t| 0.94   \t| 0.94     \t| 332     \t|","metadata":{}},{"cell_type":"code","source":"# Importing dependencies\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom catboost import CatBoostClassifier, Pool","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#e74c3c;\"> Reading </span> Data\n","metadata":{}},{"cell_type":"code","source":"# Reading data and cleaning, renaming and other data cleaning applications\n\ndata1 = pd.read_csv('/kaggle/input/telangana-post-monsoon-ground-water-quality-data/ground_water_quality_2018_post.csv')\ndata2 = pd.read_csv('/kaggle/input/telangana-post-monsoon-ground-water-quality-data/ground_water_quality_2019_post.csv')\ndata3 = pd.read_csv('/kaggle/input/telangana-post-monsoon-ground-water-quality-data/ground_water_quality_2020_post.csv')\n\n\n\ndata2.rename( columns ={ 'EC' : 'E.C', 'CO_-2 ' : 'CO3', 'HCO_ - ' :'HCO3', 'Cl -' : 'Cl',\n                        'F -' : 'F', 'NO3- ': 'NO3 ' , 'SO4-2':'SO4' , 'Na+':'Na', 'K+':'K',\n                        'Ca+2' : 'Ca', 'Mg+2':'Mg'}, inplace = True)\n\n\n# dropping redundant columns\ndata1.drop(['sno','season'], axis = 1, inplace = True)\ndata2.drop(['sno','season'], axis = 1, inplace = True)\ndata3.drop(['sno','Unnamed: 8', 'season'], axis = 1, inplace = True)\n\n\n# creating new columns\ndata1['year'] = 2018\ndata2['year'] = 2019\ndata3['year'] = 2020\n\n\n# handling and fixing outliers\ndata3['pH'].iloc[261] = data3['pH'].iloc[261].replace('8..05', '8.05')\ndata3['pH'] = data3['pH'].apply(pd.to_numeric)\n\ndata3['Classification'].iloc[178] = data3['Classification'].iloc[178].replace('O.G', 'OG')\ndata3['Classification'].iloc[208] = data3['Classification'].iloc[208].replace('O.G', 'OG')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating and applying the new_class function\n\ndef new_class(X):\n    if (X == 'C3S4') | (X == 'C2S2') | (X == 'C4S4') | (X == 'C3S3') | (X == 'C4S3') | (X == 'OG')  | (X == 'C1S1')  :\n        return 'Other'\n    else:\n        return X\n    \ndata1['Classification'] = data1['Classification'].apply(new_class)\ndata2['Classification'] = data2['Classification'].apply(new_class)\ndata3['Classification'] = data3['Classification'].apply(new_class)\n\ndata_full = pd.concat([data1, data2, data3], axis = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total null elements\n\ndata_full.isnull().sum()[data_full.isnull().sum() > 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing null values\n\nimp_knn = KNNImputer(n_neighbors=3)\n\ndata_full['CO3'] = imp_knn.fit_transform(np.array(data_full['CO3']).reshape(-1,1) )\ndata_full['gwl'] = imp_knn.fit_transform(np.array(data_full['gwl']).reshape(-1,1) )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_full.isnull().sum()[data_full.isnull().sum() > 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_full.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating train data and target\n\nX = data_full.copy()\nX.drop('Classification', axis= 1, inplace = True)\n\ny = data_full['Classification']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LB = LabelEncoder()\ny = LB.fit_transform(y)\nLB.classes_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical features\n\ncat_feat_idx =  np.where(X.dtypes == 'object')[0]\ncat_feat_idx","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scaling numerical data\n\nMX = MinMaxScaler()\nX.iloc[:, 3:21] = MX.fit_transform(X.iloc[:, 3:21])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3 , shuffle = True, stratify=y , random_state= 2)\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating class weights\n\nunique_classes = np.unique(y)\nweights = compute_class_weight(class_weight='balanced', classes=unique_classes, y=y_train)\nclass_weights = dict(zip(unique_classes, weights))\nclass_weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#e74c3c;\"> CatBoost </span> Classifier\n","metadata":{}},{"cell_type":"code","source":"# creating pools for training and testing\n\ntrain_pool = Pool(X_train, y_train, cat_features = cat_feat_idx)\ntest_pool = Pool(X_test, y_test, cat_features = cat_feat_idx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tuned with optima\n\nmodel = CatBoostClassifier(iterations= 14400,learning_rate =0.0029536992550707585 , min_data_in_leaf = 27 , class_weights=class_weights)\n\nmodel.fit(train_pool , verbose = 1000 )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#e74c3c;\"> Results </span> \n","metadata":{}},{"cell_type":"code","source":"# predictions and scores\n\npred = model.predict(test_pool)\n\nr2_sr = r2_score(y_test, pred)\nmse = mean_squared_error(y_test, pred)\n\nprint('R2 Score :{0:.5f}'.format(r2_sr))\nprint('Mean Squared Error :{0:.5f}'.format(mse))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\n\nclf_report = classification_report(pred, y_test )\n\nprint(clf_report)","metadata":{},"execution_count":null,"outputs":[]}]}